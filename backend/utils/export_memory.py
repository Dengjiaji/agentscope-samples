#!/usr/bin/env python3
"""
å¯¼å‡ºè„šæœ¬ï¼šSQLite -> JSONL
ä»Ž ChromaDB SQLite å¯¼å‡ºæ‰€æœ‰æ•°æ®åˆ° .jsonl æ–‡ä»¶
"""
import os
import sys
from pathlib import Path

BASE_DIR = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(BASE_DIR))

from dotenv import load_dotenv
load_dotenv()

import chromadb
from chromadb.config import Settings
from flowllm.storage.vector_store import ChromaVectorStore
from flowllm.embedding_model import OpenAICompatibleEmbeddingModel
from backend.config.path_config import get_logs_and_memory_dir


def export_sqlite_to_jsonl(config_name: str = "mock", output_dir: str = None):
    """ä»Ž SQLite å¯¼å‡ºåˆ° JSONL"""
    
    # æºç›®å½•ï¼ˆSQLite ä½ç½®ï¼‰
    sqlite_dir = get_logs_and_memory_dir() / config_name / "memory_data" / "reme_vector_store"
    sqlite_file = sqlite_dir / "chroma.sqlite3"
    
    if not sqlite_file.exists():
        print(f"âŒ SQLite database not found: {sqlite_file}")
        return
    
    print(f"ðŸ—„ï¸  SQLite database: {sqlite_file}")
    print(f"ðŸ“‚ Size: {sqlite_file.stat().st_size:,} bytes\n")
    
    # è¾“å‡ºç›®å½•
    if output_dir is None:
        output_dir = sqlite_dir / "exported_jsonl"
    else:
        output_dir = Path(output_dir)
    
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"ðŸ“ Output directory: {output_dir}\n")
    
    # åˆ›å»º embedding model
    embedding_model = OpenAICompatibleEmbeddingModel(
        dimensions=int(os.getenv("REME_EMBEDDING_DIMENSIONS", "64")),
        model_name=os.getenv("MEMORY_EMBEDDING_MODEL", "text-embedding-v4")
    )
    
    # åˆ›å»º vector store
    vector_store = ChromaVectorStore(
        embedding_model=embedding_model,
        store_dir=str(sqlite_dir),
        batch_size=1024
    )
    
    # æ›¿æ¢ä¸º PersistentClient è¯»å– SQLite
    vector_store.collections.clear()
    vector_store._client = chromadb.PersistentClient(
        path=str(sqlite_dir),
        settings=Settings(anonymized_telemetry=False)
    )
    
    # åˆ—å‡ºæ‰€æœ‰ collections (workspaces)
    all_collections = vector_store._client.list_collections()
    
    if not all_collections:
        print("âŒ No workspaces found in SQLite database")
        return
    
    print(f"ðŸ“Š Found {len(all_collections)} workspaces\n")
    
    # é€ä¸ªå¯¼å‡º
    total_nodes = 0
    for collection in all_collections:
        workspace_id = collection.name
        node_count = collection.count()
        
        print(f"ðŸ“¤ Exporting: workspace '{workspace_id}' ({node_count} nodes)")
        
        try:
            # å¯¼å‡ºåˆ° JSONL
            output_file = output_dir / f"{workspace_id}.jsonl"
            vector_store.dump_workspace(workspace_id, path=str(output_dir))
            
            if output_file.exists():
                file_size = output_file.stat().st_size
                print(f"   âœ… Saved to: {output_file.name} ({file_size:,} bytes)\n")
                total_nodes += node_count
            else:
                print(f"   âš ï¸  File not created\n")
                
        except Exception as e:
            print(f"   âŒ Error: {e}\n")
    
    # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
    print(f"=" * 60)
    print(f"âœ… Export complete!")
    print(f"ðŸ“Š Total: {len(all_collections)} workspaces, {total_nodes} nodes")
    print(f"ðŸ“ Exported to: {output_dir}")
    print(f"=" * 60)


if __name__ == "__main__":
    # å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æŒ‡å®š config_name å’Œè¾“å‡ºç›®å½•
    config_name = sys.argv[1] if len(sys.argv) > 1 else "mock"
    output_dir = sys.argv[2] if len(sys.argv) > 2 else None
    
    print(f"ðŸš€ Starting SQLite -> JSONL export")
    print(f"ðŸ“¦ Config: {config_name}\n")
    
    export_sqlite_to_jsonl(config_name, output_dir)

