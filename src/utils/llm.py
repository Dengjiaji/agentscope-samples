"""Helper functions for LLM"""

import json
from pydantic import BaseModel
from typing import Optional, Dict, Any, Union

# å¯¼å…¥ AgentScope æ¨¡åž‹
from src.llm.agentscope_models import get_model as get_agentscope_model, ModelProvider
from src.utils.progress import progress
from src.graph.state import AgentState


def call_llm(
    prompt: Union[str, list],
    pydantic_model: type[BaseModel],
    agent_name: Optional[str] = None,
    state: Optional[AgentState] = None,
    max_retries: int = 3,
    default_factory=None,
) -> BaseModel:
    """
    ä½¿ç”¨ AgentScope æ¨¡åž‹åŒ…è£…å™¨è°ƒç”¨ LLMï¼Œæ”¯æŒç»“æž„åŒ–è¾“å‡º
    
    Args:
        prompt: æç¤ºå†…å®¹ï¼ˆå­—ç¬¦ä¸²æˆ–æ¶ˆæ¯åˆ—è¡¨ï¼‰
        pydantic_model: Pydantic æ¨¡åž‹ç±»ç”¨äºŽç»“æž„åŒ–è¾“å‡º
        agent_name: Agent åç§°ï¼ˆå¯é€‰ï¼Œç”¨äºŽè¿›åº¦æ›´æ–°å’Œæ¨¡åž‹é…ç½®æå–ï¼‰
        state: AgentState å¯¹è±¡ï¼ˆå¯é€‰ï¼Œç”¨äºŽæå– agent ç‰¹å®šçš„æ¨¡åž‹é…ç½®ï¼‰
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤: 3ï¼‰
        default_factory: é»˜è®¤å“åº”å·¥åŽ‚å‡½æ•°ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        Pydantic æ¨¡åž‹å®žä¾‹
    """
    
    # æå–æ¨¡åž‹é…ç½®
    if state and agent_name:
        model_name, model_provider = get_agent_model_config(state, agent_name)
    else:
        # ä½¿ç”¨ç³»ç»Ÿé»˜è®¤é…ç½®
        model_name = "gpt-4o-mini"
        model_provider = "OPENAI"

    # æå– API keys
    api_keys = None
    if state:
        request = state.get("metadata", {}).get("request")
        if request and hasattr(request, 'api_keys'):
            api_keys = request.api_keys

    # èŽ·å–æ¨¡åž‹å®žä¾‹ï¼ˆä½¿ç”¨ AgentScopeï¼‰
    llm = get_agentscope_model(model_name, model_provider, api_keys)

    # å‡†å¤‡ promptï¼ˆæ·»åŠ  JSON æ ¼å¼è¦æ±‚ï¼‰
    if isinstance(prompt, str):
        json_schema = pydantic_model.model_json_schema()
        enhanced_prompt = f"""{prompt}

è¯·ä»¥ JSON æ ¼å¼è¿”å›žç»“æžœï¼Œä¸¥æ ¼éµå¾ªä»¥ä¸‹ schemaï¼š
{json.dumps(json_schema, indent=2, ensure_ascii=False)}

åªè¿”å›ž JSONï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—ã€‚"""
        messages = [{"role": "user", "content": enhanced_prompt}]
    else:
        messages = prompt

    # è°ƒç”¨ LLMï¼ˆå¸¦é‡è¯•é€»è¾‘ï¼‰
    for attempt in range(max_retries):
        try:
            # ä½¿ç”¨ AgentScope æ¨¡åž‹
            response = llm(
                messages,
                temperature=0.7,
                response_format={"type": "json_object"} if model_provider == ModelProvider.OPENAI else None
            )
            content = response["content"]

            # è§£æž JSON å“åº”
            parsed_result = extract_json_from_response(content)
            if parsed_result:
                return pydantic_model(**parsed_result)
            
            # å¦‚æžœè§£æžå¤±è´¥ï¼Œå°è¯•ç›´æŽ¥è§£æž
            try:
                return pydantic_model(**json.loads(content))
            except:
                pass

        except Exception as e:
            # æ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯
            error_details = f"LLM Error - Agent: {agent_name}, Model: {model_name} ({model_provider}), Attempt: {attempt + 1}/{max_retries}"
            print(f"{error_details}")
            print(f"Error Type: {type(e).__name__}")
            print(f"Error Message: {str(e)}")
            
            import traceback
            print(f"Full Traceback:\n{traceback.format_exc()}")
            
            # if agent_name:
            #     progress.update_status(agent_name, None, f"Error - retry {attempt + 1}/{max_retries}: {type(e).__name__}")

            if attempt == max_retries - 1:
                print(f"ðŸš¨ FINAL ERROR: LLM call failed after {max_retries} attempts")
                print(f"ðŸš¨ Agent: {agent_name}, Model: {model_name} ({model_provider})")
                print(f"ðŸš¨ Final Error: {e}")
                
                # ä½¿ç”¨ default_factory æˆ–åˆ›å»ºé»˜è®¤å“åº”
                if default_factory:
                    return default_factory()
                return create_default_response(pydantic_model)

    # ä¸åº”è¯¥åˆ°è¾¾è¿™é‡Œ
    return create_default_response(pydantic_model)


def create_default_response(model_class: type[BaseModel]) -> BaseModel:
    """Creates a safe default response based on the model's fields."""
    default_values = {}
    for field_name, field in model_class.model_fields.items():
        if field.annotation == str:
            default_values[field_name] = "Error in analysis, using default"
        elif field.annotation == float:
            default_values[field_name] = 0.0
        elif field.annotation == int:
            default_values[field_name] = 0
        elif hasattr(field.annotation, "__origin__") and field.annotation.__origin__ == dict:
            default_values[field_name] = {}
        else:
            # For other types (like Literal), try to use the first allowed value
            if hasattr(field.annotation, "__args__"):
                default_values[field_name] = field.annotation.__args__[0]
            else:
                default_values[field_name] = None

    return model_class(**default_values)


def extract_json_from_response(content: str) -> dict | None:
    """Extracts JSON from markdown-formatted response."""
    try:
        json_start = content.find("```json")
        if json_start != -1:
            json_text = content[json_start + 7 :]  # Skip past ```json
            json_end = json_text.find("```")
            if json_end != -1:
                json_text = json_text[:json_end].strip()
                return json.loads(json_text)
    except Exception as e:
        print(f"Error extracting JSON from response: {e}")
    return None


def get_agent_model_config(state, agent_name):
    """
    Get model configuration for a specific agent from the state.
    Falls back to global model configuration if agent-specific config is not available.
    Always returns valid model_name and model_provider values.
    """
    request = state.get("metadata", {}).get("request")
    
    if request and hasattr(request, 'get_agent_model_config'):
        # Get agent-specific model configuration
        model_name, model_provider = request.get_agent_model_config(agent_name)
        # Ensure we have valid values
        if model_name and model_provider:
            return model_name, model_provider.value if hasattr(model_provider, 'value') else str(model_provider)
    
    # Fall back to global configuration (system defaults)
    model_name = state.get("metadata", {}).get("model_name") or "gpt-4.1"
    model_provider = state.get("metadata", {}).get("model_provider") or "OPENAI"
    
    # Convert enum to string if necessary
    if hasattr(model_provider, 'value'):
        model_provider = model_provider.value
    
    return model_name, model_provider
