#!/usr/bin/env python3
"""
è‡ªåŠ¨å¢é‡æ›´æ–°å†å²æ•°æ®æ¨¡å—

åŠŸèƒ½:
1. ä» Finnhub API è·å–è‚¡ç¥¨å†å²æ•°æ®
2. å¢é‡æ›´æ–° ret_data ç›®å½•ä¸­çš„ CSV æ–‡ä»¶
3. è‡ªåŠ¨æ£€æµ‹æœ€åæ›´æ–°æ—¥æœŸ,åªä¸‹è½½æ–°æ•°æ®
4. è®¡ç®—æ”¶ç›Šç‡ (ret)
5. æ”¯æŒæ‰¹é‡æ›´æ–°å¤šä¸ªè‚¡ç¥¨
"""

import os
import sys
from pathlib import Path
from datetime import datetime, timedelta
import pandas as pd
import logging
from typing import List, Optional, Dict
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
BASE_DIR = Path(__file__).resolve().parents[2]
if str(BASE_DIR) not in sys.path:
    sys.path.append(str(BASE_DIR))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s'
)
logger = logging.getLogger(__name__)


class DataUpdater:
    """æ•°æ®æ›´æ–°å™¨"""
    
    def __init__(
        self, 
        api_key: str,
        data_dir: str = None,
        start_date: str = "2022-01-01"
    ):
        """
        åˆå§‹åŒ–æ•°æ®æ›´æ–°å™¨
        
        Args:
            api_key: Finnhub API key
            data_dir: æ•°æ®å­˜å‚¨ç›®å½•,é»˜è®¤ä¸º src/data/ret_data
            start_date: å†å²æ•°æ®èµ·å§‹æ—¥æœŸ (YYYY-MM-DD)
        """
        self.api_key = api_key
        
        # è®¾ç½®æ•°æ®ç›®å½•
        if data_dir is None:
            self.data_dir = BASE_DIR / "src" / "data" / "ret_data"
        else:
            self.data_dir = Path(data_dir)
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        self.start_date = start_date
        
        # å»¶è¿Ÿå¯¼å…¥ finnhub (é¿å…åœ¨æ²¡æœ‰å®‰è£…æ—¶æŠ¥é”™)
        try:
            import finnhub
            self.finnhub = finnhub
            self.client = finnhub.Client(api_key=api_key)
            logger.info("âœ… Finnhub å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except ImportError:
            logger.error("âŒ æœªå®‰è£… finnhub-python åŒ…,è¯·è¿è¡Œ: pip install finnhub-python")
            raise
    
    def get_last_date_from_csv(self, ticker: str) -> Optional[datetime]:
        """
        ä» CSV æ–‡ä»¶ä¸­è·å–æœ€åä¸€æ¡æ•°æ®çš„æ—¥æœŸ
        
        Args:
            ticker: è‚¡ç¥¨ä»£ç 
            
        Returns:
            æœ€åæ—¥æœŸçš„ datetime å¯¹è±¡,å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨è¿”å› None
        """
        csv_path = self.data_dir / f"{ticker}.csv"
        
        if not csv_path.exists():
            logger.info(f"ğŸ“‚ {ticker}.csv ä¸å­˜åœ¨,å°†åˆ›å»ºæ–°æ–‡ä»¶")
            return None
        
        try:
            df = pd.read_csv(csv_path)
            if df.empty or 'time' not in df.columns:
                return None
            
            # è·å–æœ€åä¸€è¡Œçš„æ—¥æœŸ
            last_date_str = df['time'].iloc[-1]
            last_date = datetime.strptime(last_date_str, '%Y-%m-%d')
            logger.info(f"ğŸ“… {ticker} æœ€åæ•°æ®æ—¥æœŸ: {last_date_str}")
            return last_date
        except Exception as e:
            logger.warning(f"âš ï¸ è¯»å– {ticker}.csv å¤±è´¥: {e}")
            return None
    
    def fetch_data_from_api(
        self, 
        ticker: str, 
        start_date: datetime, 
        end_date: datetime
    ) -> Optional[pd.DataFrame]:
        """
        ä» Finnhub API è·å–æ•°æ®
        
        Args:
            ticker: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            
        Returns:
            DataFrame æˆ– None
        """
        try:
            start_timestamp = int(start_date.timestamp())
            end_timestamp = int(end_date.timestamp())
            
            logger.info(f"ğŸ”„ æ­£åœ¨è·å– {ticker} æ•°æ®: {start_date.date()} åˆ° {end_date.date()}")
            
            # è°ƒç”¨ API
            data = self.client.stock_candles(
                ticker, 
                'D',  # æ—¥çº¿æ•°æ®
                start_timestamp, 
                end_timestamp
            )
            
            # æ£€æŸ¥è¿”å›çŠ¶æ€
            if data.get('s') != 'ok':
                logger.warning(f"âš ï¸ {ticker} API è¿”å›çŠ¶æ€å¼‚å¸¸: {data.get('s')}")
                return None
            
            # è½¬æ¢ä¸º DataFrame
            df = pd.DataFrame(data)
            
            # é‡å‘½ååˆ—
            df = df.rename(columns={
                'o': 'open',
                'c': 'close',
                'h': 'high',
                'l': 'low',
                'v': 'volume',
                't': 'timestamp'
            })
            
            # è½¬æ¢æ—¶é—´æˆ³
            df['Date'] = pd.to_datetime(df['timestamp'], unit='s', utc=True)
            df['time'] = df['Date'].dt.strftime('%Y-%m-%d')
            
            # è®¡ç®—æ”¶ç›Šç‡ (ä¸‹ä¸€æ—¥æ”¶ç›Šç‡)
            df['ret'] = df['close'].pct_change().shift(-1)
            
            # é€‰æ‹©éœ€è¦çš„åˆ—
            df = df[['open', 'close', 'high', 'low', 'volume', 'time', 'ret']]
            
            logger.info(f"âœ… æˆåŠŸè·å– {ticker} æ•°æ®: {len(df)} æ¡è®°å½•")
            return df
            
        except Exception as e:
            logger.error(f"âŒ è·å– {ticker} æ•°æ®å¤±è´¥: {e}")
            return None
    
    def merge_and_save(
        self, 
        ticker: str, 
        new_data: pd.DataFrame
    ) -> bool:
        """
        åˆå¹¶æ–°æ—§æ•°æ®å¹¶ä¿å­˜
        
        Args:
            ticker: è‚¡ç¥¨ä»£ç 
            new_data: æ–°æ•°æ® DataFrame
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        csv_path = self.data_dir / f"{ticker}.csv"
        
        try:
            if csv_path.exists():
                # è¯»å–ç°æœ‰æ•°æ®
                old_data = pd.read_csv(csv_path)
                logger.info(f"ğŸ“Š {ticker} ç°æœ‰æ•°æ®: {len(old_data)} æ¡")
                
                # åˆå¹¶æ•°æ® (å»é‡)
                combined = pd.concat([old_data, new_data], ignore_index=True)
                combined = combined.drop_duplicates(subset=['time'], keep='last')
                combined = combined.sort_values('time').reset_index(drop=True)
                
                # é‡æ–°è®¡ç®—æ”¶ç›Šç‡ (ç¡®ä¿è¿ç»­æ€§)
                combined['ret'] = combined['close'].pct_change().shift(-1)
                
                logger.info(f"ğŸ“Š {ticker} åˆå¹¶åæ•°æ®: {len(combined)} æ¡")
            else:
                combined = new_data
                logger.info(f"ğŸ“Š {ticker} æ–°å»ºæ–‡ä»¶: {len(combined)} æ¡")
            
            # ä¿å­˜åˆ° CSV
            combined.to_csv(csv_path, index=False)
            logger.info(f"ğŸ’¾ {ticker} æ•°æ®å·²ä¿å­˜åˆ°: {csv_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ {ticker} æ•°æ®å¤±è´¥: {e}")
            return False
    
    def update_ticker(
        self, 
        ticker: str, 
        force_full_update: bool = False
    ) -> bool:
        """
        æ›´æ–°å•ä¸ªè‚¡ç¥¨çš„æ•°æ®
        
        Args:
            ticker: è‚¡ç¥¨ä»£ç 
            force_full_update: æ˜¯å¦å¼ºåˆ¶å…¨é‡æ›´æ–°
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ“ˆ å¼€å§‹æ›´æ–° {ticker}")
        logger.info(f"{'='*60}")
        
        # ç¡®å®šèµ·å§‹æ—¥æœŸ
        if force_full_update:
            start_date = datetime.strptime(self.start_date, '%Y-%m-%d')
            logger.info(f"ğŸ”„ å¼ºåˆ¶å…¨é‡æ›´æ–°,èµ·å§‹æ—¥æœŸ: {start_date.date()}")
        else:
            last_date = self.get_last_date_from_csv(ticker)
            if last_date:
                # ä»æœ€åæ—¥æœŸçš„ä¸‹ä¸€å¤©å¼€å§‹æ›´æ–°
                start_date = last_date + timedelta(days=1)
                logger.info(f"ğŸ“… å¢é‡æ›´æ–°,èµ·å§‹æ—¥æœŸ: {start_date.date()}")
            else:
                start_date = datetime.strptime(self.start_date, '%Y-%m-%d')
                logger.info(f"ğŸ“… é¦–æ¬¡æ›´æ–°,èµ·å§‹æ—¥æœŸ: {start_date.date()}")
        
        # ç»“æŸæ—¥æœŸä¸ºä»Šå¤©
        end_date = datetime.now()
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
        if start_date.date() >= end_date.date():
            logger.info(f"âœ… {ticker} æ•°æ®å·²æ˜¯æœ€æ–°,æ— éœ€æ›´æ–°")
            return True
        
        # è·å–æ–°æ•°æ®
        new_data = self.fetch_data_from_api(ticker, start_date, end_date)
        
        if new_data is None or new_data.empty:
            # æ£€æŸ¥æ˜¯å¦æ˜¯å‘¨æœ«æˆ–æœ€è¿‘çš„æ—¥æœŸï¼ˆå¯èƒ½æ˜¯æ•°æ®å»¶è¿Ÿï¼‰
            days_diff = (end_date - start_date).days
            if days_diff <= 3:  # å¦‚æœåªå·®1-3å¤©ï¼Œå¯èƒ½æ˜¯å‘¨æœ«æˆ–æ•°æ®å»¶è¿Ÿ
                logger.info(f"â„¹ï¸ {ticker} æš‚æ— æ–°æ•°æ® (å¯èƒ½æ˜¯å‘¨æœ«/å‡æœŸ/æ•°æ®å»¶è¿Ÿ)ï¼Œç°æœ‰æ•°æ®å·²è¶³å¤Ÿ")
                return True  # è¿”å›æˆåŠŸï¼Œè®©è„šæœ¬ç»§ç»­
            else:
                logger.warning(f"âš ï¸ {ticker} æ²¡æœ‰æ–°æ•°æ®")
                return False
        
        # åˆå¹¶å¹¶ä¿å­˜
        success = self.merge_and_save(ticker, new_data)
        
        if success:
            logger.info(f"âœ… {ticker} æ›´æ–°å®Œæˆ")
        else:
            logger.error(f"âŒ {ticker} æ›´æ–°å¤±è´¥")
        
        return success
    
    def update_all_tickers(
        self, 
        tickers: List[str], 
        force_full_update: bool = False
    ) -> Dict[str, bool]:
        """
        æ‰¹é‡æ›´æ–°å¤šä¸ªè‚¡ç¥¨
        
        Args:
            tickers: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            force_full_update: æ˜¯å¦å¼ºåˆ¶å…¨é‡æ›´æ–°
            
        Returns:
            æ›´æ–°ç»“æœå­—å…¸ {ticker: success}
        """
        results = {}
        
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡æ›´æ–° {len(tickers)} åªè‚¡ç¥¨")
        logger.info(f"ğŸ“‹ è‚¡ç¥¨åˆ—è¡¨: {', '.join(tickers)}")
        logger.info(f"{'='*60}\n")
        
        for i, ticker in enumerate(tickers, 1):
            logger.info(f"\n[{i}/{len(tickers)}] å¤„ç† {ticker}")
            results[ticker] = self.update_ticker(ticker, force_full_update)
            
            # API é™æµ (Finnhub å…è´¹ç‰ˆæœ‰é™åˆ¶)
            if i < len(tickers):
                import time
                time.sleep(1)  # æ¯æ¬¡è¯·æ±‚é—´éš” 1 ç§’
        
        # æ‰“å°æ±‡æ€»
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ“Š æ›´æ–°æ±‡æ€»")
        logger.info(f"{'='*60}")
        
        success_count = sum(results.values())
        fail_count = len(results) - success_count
        
        logger.info(f"âœ… æˆåŠŸ: {success_count}")
        logger.info(f"âŒ å¤±è´¥: {fail_count}")
        
        if fail_count > 0:
            failed_tickers = [t for t, s in results.items() if not s]
            logger.warning(f"å¤±è´¥çš„è‚¡ç¥¨: {', '.join(failed_tickers)}")
        
        logger.info(f"{'='*60}\n")
        
        return results


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description='è‡ªåŠ¨æ›´æ–°è‚¡ç¥¨å†å²æ•°æ®')
    parser.add_argument(
        '--tickers',
        type=str,
        help='è‚¡ç¥¨ä»£ç åˆ—è¡¨ (é€—å·åˆ†éš”),ä¾‹å¦‚: AAPL,MSFT,GOOGL'
    )
    parser.add_argument(
        '--api-key',
        type=str,
        help='Finnhub API Key (ä¹Ÿå¯é€šè¿‡ FINNHUB_API_KEY ç¯å¢ƒå˜é‡è®¾ç½®)'
    )
    parser.add_argument(
        '--data-dir',
        type=str,
        help='æ•°æ®å­˜å‚¨ç›®å½• (é»˜è®¤: src/data/ret_data)'
    )
    parser.add_argument(
        '--start-date',
        type=str,
        default='2022-01-01',
        help='å†å²æ•°æ®èµ·å§‹æ—¥æœŸ (YYYY-MM-DD,é»˜è®¤: 2022-01-01)'
    )
    parser.add_argument(
        '--force',
        action='store_true',
        help='å¼ºåˆ¶å…¨é‡æ›´æ–° (é‡æ–°ä¸‹è½½æ‰€æœ‰æ•°æ®)'
    )
    
    args = parser.parse_args()
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    
    # è·å– API Key
    api_key = args.api_key or os.getenv('FINNHUB_API_KEY')
    if not api_key:
        logger.error("âŒ æœªæä¾› Finnhub API Key")
        logger.error("   è¯·é€šè¿‡ --api-key å‚æ•°æˆ– FINNHUB_API_KEY ç¯å¢ƒå˜é‡è®¾ç½®")
        sys.exit(1)
    
    # è·å–è‚¡ç¥¨åˆ—è¡¨
    if args.tickers:
        tickers = [t.strip().upper() for t in args.tickers.split(',')]
    else:
        # ä»ç¯å¢ƒå˜é‡è¯»å–
        tickers_env = os.getenv('TICKERS', '')
        if tickers_env:
            tickers = [t.strip().upper() for t in tickers_env.split(',')]
        else:
            logger.error("âŒ æœªæä¾›è‚¡ç¥¨åˆ—è¡¨")
            logger.error("   è¯·é€šè¿‡ --tickers å‚æ•°æˆ– TICKERS ç¯å¢ƒå˜é‡è®¾ç½®")
            sys.exit(1)
    
    # åˆ›å»ºæ›´æ–°å™¨
    updater = DataUpdater(
        api_key=api_key,
        data_dir=args.data_dir,
        start_date=args.start_date
    )
    
    # æ‰§è¡Œæ›´æ–°
    results = updater.update_all_tickers(tickers, force_full_update=args.force)
    
    # è¿”å›çŠ¶æ€ç 
    success_count = sum(results.values())
    if success_count == len(results):
        logger.info("ğŸ‰ æ‰€æœ‰è‚¡ç¥¨æ›´æ–°æˆåŠŸ!")
        sys.exit(0)
    elif success_count == 0:
        # æ‰€æœ‰è‚¡ç¥¨éƒ½å¤±è´¥ï¼Œå¯èƒ½æ˜¯å‘¨æœ«/å‡æœŸ
        logger.warning("âš ï¸ æ‰€æœ‰è‚¡ç¥¨éƒ½æ— æ–°æ•°æ® (å¯èƒ½æ˜¯å‘¨æœ«/å‡æœŸ)ï¼Œå°†ä½¿ç”¨ç°æœ‰æ•°æ®")
        logger.info("ğŸ’¡ æç¤º: ç³»ç»Ÿå°†ç»§ç»­è¿è¡Œ")
        sys.exit(0)  # è¿”å›æˆåŠŸï¼Œè®©æœåŠ¡å™¨ç»§ç»­å¯åŠ¨
    else:
        # éƒ¨åˆ†æˆåŠŸéƒ¨åˆ†å¤±è´¥
        logger.warning("âš ï¸ éƒ¨åˆ†è‚¡ç¥¨æ›´æ–°å¤±è´¥ï¼Œä½†å°†ç»§ç»­è¿è¡Œ")
        sys.exit(0)  # è¿”å›æˆåŠŸï¼Œè®©æœåŠ¡å™¨ç»§ç»­å¯åŠ¨


if __name__ == '__main__':
    main()

