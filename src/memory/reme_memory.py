#!/usr/bin/env python3
"""
ReMe Long-term Memory Implementation
ä½¿ç”¨å…¨å±€å•ä¾‹ ChromaVectorStoreï¼Œé€šè¿‡ workspace_id åŒºåˆ†ä¸åŒç”¨æˆ·
"""

import os
import uuid
import logging
from typing import Dict, List, Any, Optional
from pathlib import Path

from .base import LongTermMemory

from flowllm.storage.vector_store import ChromaVectorStore
from flowllm.embedding_model import OpenAICompatibleEmbeddingModel
from flowllm.schema.vector_node import VectorNode
from src.config.path_config import get_logs_and_memory_dir

logger = logging.getLogger(__name__)

# æ¯æ¡è®°å¿†çš„æœ€å¤§å­—ç¬¦é•¿åº¦ï¼ˆtext-embedding-v4 é™åˆ¶çº¦ä¸º 8192 tokensï¼Œçº¦ç­‰äº 8000 å­—ç¬¦ï¼‰
MAX_CONTENT_LENGTH = 8000


class ReMeMemory(LongTermMemory):
    """
    ReMeé•¿æœŸè®°å¿†å®ç°
    
    è®¾è®¡ç†å¿µï¼š
    - å…¨å±€å…±äº«ä¸€ä¸ª ChromaVectorStore å®ä¾‹ï¼ˆé¿å… Chroma å†²çªï¼‰
    - ä½¿ç”¨ workspace_id = f"{base_dir}_{user_id}" åŒºåˆ†ä¸åŒé…ç½®å’Œç”¨æˆ·
    - æ‰€æœ‰æ•°æ®å­˜å‚¨åœ¨ç»Ÿä¸€çš„æ ¹ç›®å½•ä¸‹
    """
    
    # å…¨å±€å•ä¾‹ï¼šä¸€ä¸ªè¿›ç¨‹åªæœ‰ä¸€ä¸ª ChromaVectorStore
    _global_vector_store: Optional[ChromaVectorStore] = None
    _global_embedding_model: Optional[OpenAICompatibleEmbeddingModel] = None
    _global_store_dir: Optional[str] = None
    
    def __init__(self, base_dir: str):
        """
        åˆå§‹åŒ–ReMeè®°å¿†
        
        Args:
            base_dir: åŸºç¡€ç›®å½•ï¼ˆconfig_nameï¼‰ï¼Œä¼šä½œä¸º workspace_id çš„å‰ç¼€
        """
        self.base_dir = base_dir
        
        # ä½¿ç”¨å…¨å±€ç»Ÿä¸€çš„å­˜å‚¨ç›®å½•
        if ReMeMemory._global_store_dir is None:
            ReMeMemory._global_store_dir = str(get_logs_and_memory_dir() / base_dir / "memory_data" / "reme_vector_store")
            os.makedirs(ReMeMemory._global_store_dir, exist_ok=True)
        
        self.store_dir = ReMeMemory._global_store_dir
        
        # åˆå§‹åŒ–å…¨å±€ embedding æ¨¡å‹ï¼ˆåªåˆ›å»ºä¸€æ¬¡ï¼‰
        if ReMeMemory._global_embedding_model is None:
            embedding_model_name = os.getenv("MEMORY_EMBEDDING_MODEL", "text-embedding-3-small")
            embedding_dim = int(os.getenv("REME_EMBEDDING_DIMENSIONS", "1536"))
            
            logger.info(f"åˆå§‹åŒ–å…¨å±€ Embedding æ¨¡å‹: {embedding_model_name} (dim={embedding_dim})")
            ReMeMemory._global_embedding_model = OpenAICompatibleEmbeddingModel(
                dimensions=embedding_dim,
                model_name=embedding_model_name
            )
        
        # åˆå§‹åŒ–å…¨å±€å‘é‡å­˜å‚¨ï¼ˆåªåˆ›å»ºä¸€æ¬¡ï¼Œè§£å†³ Chroma å†²çªï¼‰
        if ReMeMemory._global_vector_store is None:
            logger.info(f"åˆå§‹åŒ–å…¨å±€ ChromaVectorStore: {self.store_dir}")
            ReMeMemory._global_vector_store = ChromaVectorStore(
                embedding_model=ReMeMemory._global_embedding_model,
                store_dir=self.store_dir,
                batch_size=1024
            )
        
        # å…ˆèµ‹å€¼ vector_storeï¼Œç¡®ä¿åç»­æ–¹æ³•å¯ä»¥è®¿é—®
        self.vector_store = ReMeMemory._global_vector_store
        
        # åªåœ¨ç¬¬ä¸€æ¬¡åˆ›å»ºæ—¶åŠ è½½æ‰€æœ‰å·²æœ‰workspacesï¼ˆä½¿ç”¨æ ‡å¿—ä½é¿å…é‡å¤åŠ è½½ï¼‰
        if not hasattr(ReMeMemory, '_workspaces_loaded'):
            self._load_all_existing_workspaces()
            ReMeMemory._workspaces_loaded = True
        
        logger.info(f"ReMeè®°å¿†å·²åˆå§‹åŒ– (base_dir={base_dir})")
    
    def _load_all_existing_workspaces(self):
        """åŠ è½½æ‰€æœ‰å·²æœ‰çš„workspaceè®°å¿†"""
        jsonl_files = list(Path(self.store_dir).glob("*.jsonl"))
        
        if jsonl_files:
            logger.info(f"å‘ç° {len(jsonl_files)} ä¸ªworkspaceæ–‡ä»¶ï¼Œæ­£åœ¨åŠ è½½...")
            
        for jsonl_file in jsonl_files:
            workspace_id = jsonl_file.stem
            if not self.vector_store.exist_workspace(workspace_id):
                try:
                    self.vector_store.load_workspace(workspace_id, path=self.store_dir)
                    logger.debug(f"âœ“ åŠ è½½ workspace: {workspace_id}")
                except Exception as e:
                    logger.warning(f"âœ— åŠ è½½å¤±è´¥ {workspace_id}: {e}")
    
    
    def _get_workspace_id(self, user_id: str) -> str:
        """
        ç”Ÿæˆå®Œæ•´çš„ workspace_id
        æ ¼å¼: {base_dir}__{user_id}
        è¿™æ ·å¯ä»¥åœ¨å…¨å±€ ChromaVectorStore ä¸­åŒºåˆ†ä¸åŒé…ç½®çš„ç”¨æˆ·
        """
        return f"{user_id}"
    
    def _ensure_workspace(self, user_id: str):
        """ç¡®ä¿workspaceå­˜åœ¨"""
        workspace_id = self._get_workspace_id(user_id)
        
        if not self.vector_store.exist_workspace(workspace_id):
            # å°è¯•åŠ è½½
            workspace_file = os.path.join(self.store_dir, f"{workspace_id}.jsonl")
            if os.path.exists(workspace_file):
                try:
                    self.vector_store.load_workspace(workspace_id, path=self.store_dir)
                    return
                except Exception as e:
                    logger.warning(f"åŠ è½½workspaceå¤±è´¥: {e}")
            
            # åˆ›å»ºæ–°workspace
            self.vector_store.create_workspace(workspace_id)
    
    def add(self, content: str, user_id: str, metadata: Optional[Dict[str, Any]] = None) -> str:
        """æ·»åŠ è®°å¿†
        
        å¦‚æœå†…å®¹è¶…è¿‡ MAX_CONTENT_LENGTHï¼Œä¼šè‡ªåŠ¨åˆ†å‰²æˆå¤šæ¡è®°å½•åˆ†åˆ«å­˜å‚¨
        """
        if not content or not isinstance(content, str):
            logger.warning(f"âš ï¸ [ReMeMemory] è¾“å…¥å†…å®¹ä¸ºç©ºæˆ–éå­—ç¬¦ä¸²ç±»å‹ï¼Œè·³è¿‡")
            return ""
        
        content = content.strip()
        if not content:
            logger.warning(f"âš ï¸ [ReMeMemory] è¾“å…¥å†…å®¹ä¸ºç©ºï¼ˆä»…åŒ…å«ç©ºç™½å­—ç¬¦ï¼‰ï¼Œè·³è¿‡")
            return ""
        
        content_len = len(content)
        logger.debug(f"â• [ReMeMemory] æ·»åŠ è®°å¿†: user_id={user_id}, content_len={content_len}")
        
        self._ensure_workspace(user_id)
        workspace_id = self._get_workspace_id(user_id)
        
        logger.debug(f"   workspace_id={workspace_id}")
        
        node_metadata = metadata or {}
        node_metadata['user_id'] = user_id
        node_metadata['base_dir'] = self.base_dir
        
        # å¦‚æœå†…å®¹è¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œåˆ†å‰²æˆå¤šæ¡è®°å½•
        if content_len > MAX_CONTENT_LENGTH:
            logger.info(f"   å†…å®¹é•¿åº¦ ({content_len}) è¶…è¿‡é™åˆ¶ ({MAX_CONTENT_LENGTH})ï¼Œå°†åˆ†å‰²æˆå¤šæ¡è®°å½•")
            
            # æŒ‰ MAX_CONTENT_LENGTH åˆ†å‰²å†…å®¹
            chunks = []
            for i in range(0, content_len, MAX_CONTENT_LENGTH):
                chunk = content[i:i + MAX_CONTENT_LENGTH]
                chunks.append(chunk)
            
            logger.info(f"   åˆ†å‰²æˆ {len(chunks)} æ¡è®°å½•")
            
            # ä¸ºæ¯ä¸ªç‰‡æ®µåˆ›å»ºèŠ‚ç‚¹
            nodes = []
            first_node_id = None
            for idx, chunk in enumerate(chunks):
                node_id = str(uuid.uuid4())
                if idx == 0:
                    first_node_id = node_id
                
                # åœ¨å…ƒæ•°æ®ä¸­è®°å½•è¿™æ˜¯åˆ†å‰²è®°å½•çš„ä¸€éƒ¨åˆ†
                chunk_metadata = node_metadata.copy()
                chunk_metadata['chunk_index'] = idx
                chunk_metadata['total_chunks'] = len(chunks)
                chunk_metadata['is_chunked'] = True
                
                node = VectorNode(
                    unique_id=node_id,
                    workspace_id=workspace_id,
                    content=chunk,
                    metadata=chunk_metadata
                )
                nodes.append(node)
            
            # æ‰¹é‡æ’å…¥æ‰€æœ‰èŠ‚ç‚¹
            self.vector_store.insert(nodes, workspace_id)
            self.vector_store.dump_workspace(workspace_id, path=self.store_dir)
            
            logger.debug(f"   âœ… è®°å¿†å·²æ·»åŠ ï¼ˆåˆ†å‰²æˆ {len(chunks)} æ¡ï¼‰ï¼Œç¬¬ä¸€æ¡ node_id={first_node_id}")
            logger.debug(f"   ä¿å­˜è·¯å¾„: {self.store_dir}/{workspace_id}.jsonl")
            
            return first_node_id
        else:
            # å†…å®¹é•¿åº¦æ­£å¸¸ï¼Œç›´æ¥å­˜å‚¨
            node_id = str(uuid.uuid4())
            
            node = VectorNode(
                unique_id=node_id,
                workspace_id=workspace_id,
                content=content,
                metadata=node_metadata
            )
            
            self.vector_store.insert([node], workspace_id)
            self.vector_store.dump_workspace(workspace_id, path=self.store_dir)
            
            logger.debug(f"   âœ… è®°å¿†å·²æ·»åŠ ï¼Œnode_id={node_id}")
            logger.debug(f"   ä¿å­˜è·¯å¾„: {self.store_dir}/{workspace_id}.jsonl")
            
            return node_id
    
    def search(self, query: str, user_id: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """æœç´¢è®°å¿†"""
        if not query or not isinstance(query, str):
            logger.warning(f"âš ï¸ [ReMeMemory] æœç´¢æŸ¥è¯¢ä¸ºç©ºæˆ–éå­—ç¬¦ä¸²ç±»å‹ï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return []
        
        query = query.strip()
        if not query:
            logger.warning(f"âš ï¸ [ReMeMemory] æœç´¢æŸ¥è¯¢ä¸ºç©ºï¼ˆä»…åŒ…å«ç©ºç™½å­—ç¬¦ï¼‰ï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return []
        
        # å¦‚æœæŸ¥è¯¢æ–‡æœ¬è¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œæˆªæ–­
        if len(query) > MAX_CONTENT_LENGTH:
            logger.warning(f"âš ï¸ [ReMeMemory] æœç´¢æŸ¥è¯¢é•¿åº¦ ({len(query)}) è¶…è¿‡é™åˆ¶ ({MAX_CONTENT_LENGTH})ï¼Œå°†æˆªæ–­")
            query = query[:MAX_CONTENT_LENGTH]
        
        logger.debug(f"ğŸ” [ReMeMemory] æœç´¢è®°å¿†: user_id={user_id}, query={query[:100]}...")
        
        self._ensure_workspace(user_id)
        workspace_id = self._get_workspace_id(user_id)
        
        logger.debug(f"   workspace_id={workspace_id}")
        logger.debug(f"   workspaceå­˜åœ¨: {self.vector_store.exist_workspace(workspace_id)}")
        
        if not self.vector_store.exist_workspace(workspace_id):
            logger.warning(f"   âš ï¸ workspaceä¸å­˜åœ¨ï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return []
        
        # æ£€æŸ¥workspaceä¸­çš„èŠ‚ç‚¹æ•°é‡
        try:
            all_nodes = list(self.vector_store.iter_workspace_nodes(workspace_id))
            logger.debug(f"   workspaceä¸­å…±æœ‰ {len(all_nodes)} ä¸ªèŠ‚ç‚¹")
        except Exception as e:
            logger.debug(f"   æ— æ³•ç»Ÿè®¡èŠ‚ç‚¹æ•°é‡: {e}")
        
        nodes = self.vector_store.search(query, workspace_id, top_k=top_k)
        
        logger.debug(f"   æœç´¢ç»“æœ: {len(nodes)} æ¡")
        
        return [
            {
                'id': node.unique_id,
                'content': node.content,
                'metadata': node.metadata
            }
            for node in nodes
        ]
    
    def update(self, memory_id: str, content: str, user_id: str) -> bool:
        """æ›´æ–°è®°å¿†
        
        å¦‚æœå†…å®¹è¶…è¿‡ MAX_CONTENT_LENGTHï¼Œä¼šè‡ªåŠ¨åˆ†å‰²æˆå¤šæ¡è®°å½•åˆ†åˆ«å­˜å‚¨
        æ³¨æ„ï¼šæ›´æ–°æ—¶ä¼šåˆ é™¤æ—§è®°å½•ï¼Œå¦‚æœæ—§è®°å½•æ˜¯åˆ†å‰²çš„ï¼Œéœ€è¦æ‰‹åŠ¨åˆ é™¤æ‰€æœ‰ç›¸å…³è®°å½•
        """
        if not content or not isinstance(content, str):
            logger.warning(f"âš ï¸ [ReMeMemory] æ›´æ–°å†…å®¹ä¸ºç©ºæˆ–éå­—ç¬¦ä¸²ç±»å‹ï¼Œè·³è¿‡")
            return False
        
        content = content.strip()
        if not content:
            logger.warning(f"âš ï¸ [ReMeMemory] æ›´æ–°å†…å®¹ä¸ºç©ºï¼ˆä»…åŒ…å«ç©ºç™½å­—ç¬¦ï¼‰ï¼Œè·³è¿‡")
            return False
        
        try:
            self._ensure_workspace(user_id)
            workspace_id = self._get_workspace_id(user_id)
            
            # åˆ é™¤æ—§èŠ‚ç‚¹
            self.vector_store.delete([memory_id], workspace_id)
            
            # å¦‚æœå†…å®¹è¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œåˆ†å‰²æˆå¤šæ¡è®°å½•
            content_len = len(content)
            if content_len > MAX_CONTENT_LENGTH:
                logger.info(f"   æ›´æ–°å†…å®¹é•¿åº¦ ({content_len}) è¶…è¿‡é™åˆ¶ ({MAX_CONTENT_LENGTH})ï¼Œå°†åˆ†å‰²æˆå¤šæ¡è®°å½•")
                
                # æŒ‰ MAX_CONTENT_LENGTH åˆ†å‰²å†…å®¹
                chunks = []
                for i in range(0, content_len, MAX_CONTENT_LENGTH):
                    chunk = content[i:i + MAX_CONTENT_LENGTH]
                    chunks.append(chunk)
                
                logger.info(f"   åˆ†å‰²æˆ {len(chunks)} æ¡è®°å½•")
                
                # ä¸ºæ¯ä¸ªç‰‡æ®µåˆ›å»ºèŠ‚ç‚¹ï¼ˆç¬¬ä¸€æ¡ä½¿ç”¨åŸ memory_idï¼Œå…¶ä»–åˆ›å»ºæ–° IDï¼‰
                nodes = []
                for idx, chunk in enumerate(chunks):
                    node_id = memory_id if idx == 0 else str(uuid.uuid4())
                    
                    chunk_metadata = {
                        'user_id': user_id,
                        'base_dir': self.base_dir,
                        'chunk_index': idx,
                        'total_chunks': len(chunks),
                        'is_chunked': True
                    }
                    
                    node = VectorNode(
                        unique_id=node_id,
                        workspace_id=workspace_id,
                        content=chunk,
                        metadata=chunk_metadata
                    )
                    nodes.append(node)
                
                # æ‰¹é‡æ’å…¥æ‰€æœ‰èŠ‚ç‚¹
                self.vector_store.insert(nodes, workspace_id)
            else:
                # å†…å®¹é•¿åº¦æ­£å¸¸ï¼Œç›´æ¥æ›´æ–°
                node = VectorNode(
                    unique_id=memory_id,
                    workspace_id=workspace_id,
                    content=content,
                    metadata={'user_id': user_id, 'base_dir': self.base_dir}
                )
                
                self.vector_store.insert([node], workspace_id)
            
            self.vector_store.dump_workspace(workspace_id, path=self.store_dir)
            
            return True
        except Exception as e:
            logger.error(f"æ›´æ–°è®°å¿†å¤±è´¥: {e}")
            return False
    
    def delete(self, memory_id: str, user_id: str) -> bool:
        """åˆ é™¤è®°å¿†"""
        try:
            self._ensure_workspace(user_id)
            workspace_id = self._get_workspace_id(user_id)
            self.vector_store.delete([memory_id], workspace_id)
            self.vector_store.dump_workspace(workspace_id, path=self.store_dir)
            return True
        except Exception as e:
            logger.error(f"åˆ é™¤è®°å¿†å¤±è´¥: {e}")
            return False
    
    def get_all(self, user_id: str) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰è®°å¿†"""
        self._ensure_workspace(user_id)
        workspace_id = self._get_workspace_id(user_id)
        
        if not self.vector_store.exist_workspace(workspace_id):
            return []
        
        nodes = list(self.vector_store.iter_workspace_nodes(workspace_id))
        
        return [
            {
                'id': node.unique_id,
                'content': node.content,
                'metadata': node.metadata
            }
            for node in nodes
        ]
    
    def delete_all(self, user_id: str) -> bool:
        """åˆ é™¤æ‰€æœ‰è®°å¿†"""
        try:
            workspace_id = self._get_workspace_id(user_id)
            
            if not self.vector_store.exist_workspace(workspace_id):
                logger.info(f"workspace {workspace_id} ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…ç©º")
                return True
            
            # è·å–æ‰€æœ‰èŠ‚ç‚¹IDå¹¶åˆ é™¤
            nodes = list(self.vector_store.iter_workspace_nodes(workspace_id))
            node_ids = [node.unique_id for node in nodes]
            
            if node_ids:
                self.vector_store.delete(node_ids, workspace_id)
                self.vector_store.dump_workspace(workspace_id, path=self.store_dir)
                logger.info(f"å·²æ¸…ç©ºç”¨æˆ· {user_id} (workspace {workspace_id}) çš„ {len(node_ids)} æ¡è®°å¿†")
            
            return True
        except Exception as e:
            logger.error(f"æ¸…ç©ºè®°å¿†å¤±è´¥: {e}")
            return False
    
    @classmethod
    def reset_global_store(cls):
        """é‡ç½®å…¨å±€å‘é‡å­˜å‚¨ï¼ˆä¸»è¦ç”¨äºæµ‹è¯•ï¼‰"""
        cls._global_vector_store = None
        cls._global_embedding_model = None
        cls._global_store_dir = None
        logger.info("å…¨å±€ ChromaVectorStore å·²é‡ç½®")

