#!/usr/bin/env python3
"""
ReMe è®°å¿†æ¡†æ¶é€‚é…å™¨
å°† ReMe (flowllm) æ¡†æ¶é€‚é…åˆ°ç»Ÿä¸€çš„è®°å¿†æ¥å£
"""

import os
import logging
from typing import Dict, List, Any, Optional
from pathlib import Path

from src.memory.memory_interface import MemoryInterface

# å°è¯•å¯¼å…¥ReMeç›¸å…³æ¨¡å—
try:
    from flowllm.storage.vector_store import ChromaVectorStore
    from flowllm.embedding_model import OpenAICompatibleEmbeddingModel
    from flowllm.schema.vector_node import VectorNode
    REME_AVAILABLE = True
except ImportError as e:
    REME_AVAILABLE = False
    print(f"âš ï¸ ReMeæ¡†æ¶æœªå®‰è£…: {e}")
    print("æç¤º: è¯·å®‰è£… flowllm åŒ…ä»¥ä½¿ç”¨ReMeæ¡†æ¶")


class ReMeAdapter(MemoryInterface):
    """ReMeæ¡†æ¶é€‚é…å™¨"""
    
    def __init__(self, base_dir: str):
        """
        åˆå§‹åŒ–ReMeé€‚é…å™¨
        
        Args:
            base_dir: åŸºç¡€ç›®å½• (config_name)
        """
        if not REME_AVAILABLE:
            raise ImportError("ReMeæ¡†æ¶æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨ReMeAdapter")
        
        self.base_dir = os.path.join("logs_and_memory", base_dir)
        self.store_dir = os.path.join(self.base_dir, "memory_data", "reme_vector_store")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.store_dir, exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logging()
        
        # ğŸ”§ åŠ è½½ ReMe ç¯å¢ƒå˜é‡é…ç½®
        self._load_reme_env()
        
        # åˆå§‹åŒ–embeddingæ¨¡å‹
        embedding_model_name = os.getenv("MEMORY_EMBEDDING_MODEL", "text-embedding-v4")
        embedding_dimensions = int(os.getenv("REME_EMBEDDING_DIMENSIONS", "1024"))
        
        self.logger.info(f"ReMeé…ç½®: model={embedding_model_name}, dimensions={embedding_dimensions}")
        self.logger.info(f"ReMe API: base_url={os.getenv('FLOW_EMBEDDING_BASE_URL', 'Not set')}")
        
        self.embedding_model = OpenAICompatibleEmbeddingModel(
            dimensions=embedding_dimensions,
            model_name=embedding_model_name
        )
        # åˆå§‹åŒ–å‘é‡å­˜å‚¨
        self.vector_store = ChromaVectorStore(
            embedding_model=self.embedding_model,
            store_dir=self.store_dir,
            batch_size=1024
        )
        
        # ğŸ”§ è‡ªåŠ¨åŠ è½½å·²æœ‰çš„workspaceè®°å¿†æ–‡ä»¶
        self._load_existing_workspaces()
        
        self.logger.info(f"ReMeé€‚é…å™¨å·²åˆå§‹åŒ– (å­˜å‚¨ç›®å½•: {self.store_dir})")
    
    def _load_reme_env(self):
        """
        åŠ è½½ ReMe ç¯å¢ƒå˜é‡
        ReMe ä½¿ç”¨ FLOW_ å‰ç¼€çš„ç¯å¢ƒå˜é‡ï¼Œéœ€è¦æ˜ å°„åˆ° OpenAI å…¼å®¹çš„ç¯å¢ƒå˜é‡
        """
        # å¦‚æœå·²ç»è®¾ç½®äº† FLOW_ ç¯å¢ƒå˜é‡ï¼Œæ˜ å°„åˆ° OpenAI å…¼å®¹çš„å˜é‡
        flow_embedding_api_key = os.getenv("FLOW_EMBEDDING_API_KEY")
        flow_embedding_base_url = os.getenv("FLOW_EMBEDDING_BASE_URL")
        flow_llm_api_key = os.getenv("FLOW_LLM_API_KEY")
        flow_llm_base_url = os.getenv("FLOW_LLM_BASE_URL")
        
        # æ˜ å°„ FLOW_ å˜é‡åˆ° OPENAI_ å˜é‡ï¼ˆå¦‚æœ OPENAI_ å˜é‡æœªè®¾ç½®ï¼‰
        if flow_embedding_api_key and not os.getenv("OPENAI_API_KEY"):
            os.environ["OPENAI_API_KEY"] = flow_embedding_api_key
            self.logger.info("å·²ä» FLOW_EMBEDDING_API_KEY è®¾ç½® OPENAI_API_KEY")
        
        if flow_embedding_base_url and not os.getenv("OPENAI_BASE_URL"):
            os.environ["OPENAI_BASE_URL"] = flow_embedding_base_url
            self.logger.info(f"å·²ä» FLOW_EMBEDDING_BASE_URL è®¾ç½® OPENAI_BASE_URL: {flow_embedding_base_url}")
        
        # æ£€æŸ¥å¿…éœ€çš„ç¯å¢ƒå˜é‡
        if not os.getenv("OPENAI_API_KEY"):
            self.logger.warning("âš ï¸ æœªè®¾ç½® OPENAI_API_KEY æˆ– FLOW_EMBEDDING_API_KEY")
            self.logger.warning("   ReMe éœ€è¦ API key æ‰èƒ½ç”Ÿæˆ embedding")
        
        if not os.getenv("OPENAI_BASE_URL"):
            self.logger.warning("âš ï¸ æœªè®¾ç½® OPENAI_BASE_URL æˆ– FLOW_EMBEDDING_BASE_URL")
            self.logger.warning("   å°†ä½¿ç”¨é»˜è®¤ OpenAI API: https://api.openai.com/v1")
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
    
    def _load_existing_workspaces(self):
        """
        åŠ è½½å·²æœ‰çš„workspaceè®°å¿†æ–‡ä»¶
        éå†store_dirä¸­çš„æ‰€æœ‰.jsonlæ–‡ä»¶ï¼Œè‡ªåŠ¨åŠ è½½åˆ°å¯¹åº”çš„workspace
        """
        if not os.path.exists(self.store_dir):
            self.logger.info("å­˜å‚¨ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡åŠ è½½å·²æœ‰è®°å¿†")
            return
        
        # æŸ¥æ‰¾æ‰€æœ‰.jsonlæ–‡ä»¶
        jsonl_files = list(Path(self.store_dir).glob("*.jsonl"))
        
        if not jsonl_files:
            self.logger.info("æœªæ‰¾åˆ°å·²æœ‰çš„è®°å¿†æ–‡ä»¶")
            return
        
        loaded_count = 0
        for jsonl_file in jsonl_files:
            try:
                # ä»æ–‡ä»¶åæå–workspace_idï¼ˆå»æ‰.jsonlåç¼€ï¼‰
                workspace_id = jsonl_file.stem
                
                # æ£€æŸ¥workspaceæ˜¯å¦å·²å­˜åœ¨
                if self.vector_store.exist_workspace(workspace_id):
                    self.logger.debug(f"Workspaceå·²å­˜åœ¨ï¼Œè·³è¿‡: {workspace_id}")
                    continue
                
                # åŠ è½½workspace
                self.logger.info(f"ğŸ“¥ åŠ è½½å·²æœ‰è®°å¿†: {workspace_id} <- {jsonl_file}")
                # âš ï¸ load_workspaceçš„pathå‚æ•°åº”è¯¥æ˜¯ç›®å½•è·¯å¾„ï¼Œä¸æ˜¯æ–‡ä»¶è·¯å¾„
                # ReMeä¼šè‡ªåŠ¨åœ¨pathä¸‹æŸ¥æ‰¾ {workspace_id}.jsonl æ–‡ä»¶
                self.vector_store.load_workspace(workspace_id, path=self.store_dir)
                loaded_count += 1
                
            except Exception as e:
                self.logger.warning(f"åŠ è½½è®°å¿†æ–‡ä»¶å¤±è´¥ {jsonl_file}: {e}")
        
        if loaded_count > 0:
            self.logger.info(f"âœ… æˆåŠŸåŠ è½½ {loaded_count} ä¸ªworkspaceçš„å·²æœ‰è®°å¿†")
        else:
            self.logger.info("æœªåŠ è½½ä»»ä½•å·²æœ‰è®°å¿†")
    
    def _get_workspace_id(self, user_id: str) -> str:
        """è·å–workspace IDï¼Œç›´æ¥ä½¿ç”¨user_idä½œä¸ºworkspace_id"""
        return user_id
    
    def _load_workspace_if_exists(self, workspace_id: str):
        """
        å¦‚æœworkspaceçš„è®°å¿†æ–‡ä»¶å­˜åœ¨ä½†æœªåŠ è½½ï¼Œåˆ™å…ˆåŠ è½½
        è¿™ç¡®ä¿äº†æ¯æ¬¡æ·»åŠ è®°å¿†æ—¶éƒ½ä¼šä¿ç•™ä¹‹å‰çš„è®°å¿†
        """
        # å¦‚æœworkspaceå·²ç»åœ¨å†…å­˜ä¸­ï¼Œä¸éœ€è¦é‡æ–°åŠ è½½
        if self.vector_store.exist_workspace(workspace_id):
            return
        
        # æ£€æŸ¥å¯¹åº”çš„jsonlæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        workspace_file = os.path.join(self.store_dir, f"{workspace_id}.jsonl")
        
        if os.path.exists(workspace_file):
            try:
                self.logger.info(f"ğŸ“¥ é¦–æ¬¡ä½¿ç”¨ï¼ŒåŠ è½½å·²æœ‰è®°å¿†: {workspace_id} <- {workspace_file}")
                # âš ï¸ load_workspaceçš„pathå‚æ•°åº”è¯¥æ˜¯ç›®å½•è·¯å¾„ï¼Œä¸æ˜¯æ–‡ä»¶è·¯å¾„
                # ReMeä¼šè‡ªåŠ¨åœ¨pathä¸‹æŸ¥æ‰¾ {workspace_id}.jsonl æ–‡ä»¶
                self.vector_store.load_workspace(workspace_id, path=self.store_dir)
            except Exception as e:
                self.logger.warning(f"åŠ è½½workspaceè®°å¿†å¤±è´¥ {workspace_id}: {e}")
    
    def _ensure_workspace_exists(self, workspace_id: str):
        """ç¡®ä¿workspaceå­˜åœ¨"""
        if not self.vector_store.exist_workspace(workspace_id):
            self.vector_store.create_workspace(workspace_id)
            self.logger.info(f"åˆ›å»ºæ–°workspace: {workspace_id}")
    
    def _convert_numpy_types(self, obj):
        """
        é€’å½’è½¬æ¢å¯¹è±¡ä¸­çš„numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
        """
        try:
            import numpy as np
            
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {key: self._convert_numpy_types(value) for key, value in obj.items()}
            elif isinstance(obj, (list, tuple)):
                return [self._convert_numpy_types(item) for item in obj]
            else:
                return obj
        except ImportError:
            # å¦‚æœæ²¡æœ‰numpyï¼Œç›´æ¥è¿”å›åŸå€¼
            return obj
    
    def _normalize_metadata(self, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ ‡å‡†åŒ–metadataï¼Œå°†ä¸æ”¯æŒçš„ç±»å‹è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        ChromaDBåªæ”¯æŒ: str, int, float, bool (æ³¨æ„ï¼šå®é™…ä¸æ”¯æŒ None!)
        """
        if metadata is None:
            return {}
        
        import json
        
        # å°è¯•å¯¼å…¥numpyç”¨äºç±»å‹æ£€æŸ¥
        try:
            import numpy as np
            has_numpy = True
        except ImportError:
            has_numpy = False
        
        normalized = {}
        for key, value in metadata.items():
            # âš ï¸ è·³è¿‡ None å€¼ - ChromaDB å®é™…ä¸æ¥å— None
            if value is None:
                continue  # ä¸æ·»åŠ è¿™ä¸ªé”®å€¼å¯¹
            elif isinstance(value, (str, bool)):
                # å­—ç¬¦ä¸²å’Œå¸ƒå°”å€¼ä¿æŒä¸å˜
                normalized[key] = value
            elif isinstance(value, (int, float)):
                # PythonåŸç”Ÿæ•°å­—ç±»å‹ä¿æŒä¸å˜
                normalized[key] = value
            elif has_numpy and isinstance(value, (np.integer, np.floating)):
                # numpyæ•°å­—ç±»å‹è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹
                if isinstance(value, np.integer):
                    normalized[key] = int(value)
                else:
                    normalized[key] = float(value)
            elif isinstance(value, (list, tuple, dict)) or (has_numpy and isinstance(value, np.ndarray)):
                # å¤æ‚ç±»å‹ï¼šå…ˆè½¬æ¢numpyç±»å‹ï¼Œå†è½¬JSONå­—ç¬¦ä¸²
                try:
                    converted_value = self._convert_numpy_types(value)
                    normalized[key] = json.dumps(converted_value, ensure_ascii=False)
                except (TypeError, ValueError) as e:
                    # å¦‚æœJSONåºåˆ—åŒ–å¤±è´¥ï¼Œç›´æ¥è½¬å­—ç¬¦ä¸²
                    normalized[key] = str(value)
            else:
                # å…¶ä»–ç±»å‹è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                normalized[key] = str(value)
        
        return normalized
    
    def add(self, messages: str | List[Dict[str, Any]], user_id: str, metadata: Optional[Dict[str, Any]] = None, infer: bool = False, **kwargs) -> Dict[str, Any]:
        """
        æ·»åŠ è®°å¿†
        
        Args:
            messages: æ¶ˆæ¯å†…å®¹
            user_id: ç”¨æˆ·ID
            metadata: å…ƒæ•°æ®
            infer: Mem0å‚æ•°ï¼ŒReMeä¸­å¿½ç•¥
            **kwargs: å…¶ä»–å…¼å®¹æ€§å‚æ•°ï¼ŒReMeä¸­å¿½ç•¥
        """
        workspace_id = self._get_workspace_id(user_id)
        # ğŸ”§ å…ˆåŠ è½½å·²æœ‰è®°å¿†ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œå†ç¡®ä¿workspaceå­˜åœ¨
        self._load_workspace_if_exists(workspace_id)
        self._ensure_workspace_exists(workspace_id)
        
        # å¤„ç†æ¶ˆæ¯æ ¼å¼
        if isinstance(messages, str):
            content = messages
        elif isinstance(messages, list):
            # æå–æ¶ˆæ¯å†…å®¹
            content = "\n".join([
                f"{msg.get('role', 'user')}: {msg.get('content', '')}" 
                for msg in messages
            ])
        else:
            content = str(messages)
        
        # åˆ›å»ºVectorNode
        import uuid
        node_id = str(uuid.uuid4())
        
        node_metadata = metadata or {}
        node_metadata['user_id'] = user_id
        
        # æ ‡å‡†åŒ–metadataï¼Œç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯æ”¯æŒçš„ç±»å‹
        node_metadata = self._normalize_metadata(node_metadata)
        
        node = VectorNode(
            unique_id=node_id,
            workspace_id=workspace_id,
            content=content,
            metadata=node_metadata
        )
        
        # æ’å…¥èŠ‚ç‚¹
        self.vector_store.insert([node], workspace_id)
        
        # è‡ªåŠ¨ä¿å­˜workspace
        print(f"ğŸ’¾ å‡†å¤‡ä¿å­˜ workspace: {workspace_id}")
        print(f"   ä¿å­˜è·¯å¾„: {self.store_dir}")
        # âš ï¸ å¿…é¡»ä¼ å…¥ path å‚æ•°ï¼Œå¦åˆ™ä¼šä¿å­˜åˆ°å½“å‰å·¥ä½œç›®å½•
        self.vector_store.dump_workspace(workspace_id, path=self.store_dir)
        
        self.logger.info(f"æ·»åŠ è®°å¿†: user={user_id}, id={node_id} (å·²ä¿å­˜)")
        
        return {
            'status': 'success',
            'results': [{'id': node_id, 'memory': content}]
        }
    
    def search(self, query: str, user_id: str, top_k: int = 5, **kwargs) -> Dict[str, Any]:
        """æœç´¢è®°å¿†"""
        workspace_id = self._get_workspace_id(user_id)
        
        # ğŸ”§ å…ˆå°è¯•åŠ è½½å·²æœ‰è®°å¿†
        self._load_workspace_if_exists(workspace_id)
        
        # æ£€æŸ¥workspaceæ˜¯å¦å­˜åœ¨
        if not self.vector_store.exist_workspace(workspace_id):
            self.logger.warning(f"Workspaceä¸å­˜åœ¨: {workspace_id}")
            return {'results': []}
        
        # æœç´¢
        nodes = self.vector_store.search(query, workspace_id, top_k=top_k)
        
        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        results = []
        for node in nodes:
            result = {
                'id': node.unique_id,
                'memory': node.content,
                'metadata': node.metadata,
                'score': node.metadata.get('score', 0.0)
            }
            results.append(result)
        
        self.logger.info(f"æœç´¢è®°å¿†: user={user_id}, query='{query[:50]}...', æ‰¾åˆ°{len(results)}æ¡")
        
        return {'results': results}
    
    def update(self, memory_id: str, data: str | Dict[str, Any], workspace_id: Optional[str] = None) -> Dict[str, Any]:
        """
        æ›´æ–°è®°å¿†
        âš ï¸ ReMeæ¡†æ¶çš„æ­£ç¡®æ›´æ–°æ–¹å¼ï¼šå…ˆåˆ é™¤æ—§èŠ‚ç‚¹ï¼Œå†æ’å…¥æ–°èŠ‚ç‚¹
        
        Args:
            memory_id: è®°å¿†ID (unique_id)
            data: æ–°çš„è®°å¿†å†…å®¹
            workspace_id: workspace IDï¼Œå¦‚æœæœªæä¾›åˆ™å¿…é¡»åœ¨ data ä¸­æŒ‡å®š user_id
            
        Returns:
            æ›´æ–°ç»“æœ
        """
        # å¦‚æœæ²¡æœ‰æä¾›workspace_idï¼Œä»dataä¸­è·å–user_idå¹¶ç”Ÿæˆworkspace_id
        if workspace_id is None:
            if isinstance(data, dict) and 'user_id' in data:
                user_id = data['user_id']
                workspace_id = self._get_workspace_id(user_id)
            else:
                raise ValueError("å¿…é¡»æä¾› workspace_id æˆ–åœ¨ data ä¸­æŒ‡å®š user_id")
        
        # ğŸ”§ å…ˆåŠ è½½å·²æœ‰è®°å¿†
        self._load_workspace_if_exists(workspace_id)
        
        # å¤„ç†æ•°æ®æ ¼å¼
        if isinstance(data, str):
            content = data
            metadata = {}
        elif isinstance(data, dict):
            content = data.get('content', str(data))
            metadata = data.get('metadata', {})
        else:
            content = str(data)
            metadata = {}
        
        # æ ‡å‡†åŒ–metadata
        metadata = self._normalize_metadata(metadata)
        
        # âš ï¸ å…³é”®ä¿®å¤ï¼šReMeä¸æ”¯æŒé€šè¿‡insertè¦†ç›–ï¼Œå¿…é¡»å…ˆåˆ é™¤åæ’å…¥
        try:
            # Step 1: åˆ é™¤æ—§èŠ‚ç‚¹
            self.vector_store.delete([memory_id], workspace_id)
            self.logger.debug(f"å·²åˆ é™¤æ—§è®°å¿†èŠ‚ç‚¹: {memory_id}")
        except Exception as e:
            # å¦‚æœåˆ é™¤å¤±è´¥ï¼ˆä¾‹å¦‚èŠ‚ç‚¹ä¸å­˜åœ¨ï¼‰ï¼Œè®°å½•è­¦å‘Šä½†ç»§ç»­
            self.logger.warning(f"åˆ é™¤æ—§èŠ‚ç‚¹æ—¶å‡ºé”™ï¼ˆå¯èƒ½ä¸å­˜åœ¨ï¼‰: {e}")
        
        # Step 2: åˆ›å»ºå¹¶æ’å…¥æ–°çš„VectorNode
        updated_node = VectorNode(
            unique_id=memory_id,  # ä¿æŒç›¸åŒçš„ID
            workspace_id=workspace_id,
            content=content,
            metadata=metadata
        )
        
        self.vector_store.insert([updated_node], workspace_id)
        self.logger.debug(f"å·²æ’å…¥æ–°è®°å¿†èŠ‚ç‚¹: {memory_id}")
        
        # Step 3: è‡ªåŠ¨ä¿å­˜workspace
        self.vector_store.dump_workspace(workspace_id, path=self.store_dir)
        
        self.logger.info(f"âœ… æ›´æ–°è®°å¿†æˆåŠŸ: workspace={workspace_id}, id={memory_id} (å·²ä¿å­˜)")
        
        return {
            'status': 'success',
            'message': f'å·²æ›´æ–°è®°å¿† {memory_id}',
            'memory_id': memory_id
        }
    
    def delete(self, memory_id: str, workspace_id: Optional[str] = None) -> Dict[str, Any]:
        """
        åˆ é™¤è®°å¿†
        
        Args:
            memory_id: è®°å¿†ID
            workspace_id: workspace IDï¼Œå¿…é¡»æä¾›
            
        Returns:
            åˆ é™¤ç»“æœ
        """
        if workspace_id is None:
            raise ValueError("å¿…é¡»æä¾› workspace_id æ¥åˆ é™¤è®°å¿†")
        
        # ğŸ”§ å…ˆåŠ è½½å·²æœ‰è®°å¿†
        self._load_workspace_if_exists(workspace_id)
        
        # æ‰§è¡Œåˆ é™¤
        self.vector_store.delete([memory_id], workspace_id)
        
        # è‡ªåŠ¨ä¿å­˜workspace
        self.vector_store.dump_workspace(workspace_id, path=self.store_dir)
        
        self.logger.info(f"åˆ é™¤è®°å¿†: workspace={workspace_id}, id={memory_id} (å·²ä¿å­˜)")
        
        return {
            'status': 'success',
            'message': f'å·²åˆ é™¤è®°å¿† {memory_id}'
        }
    
    def delete_by_workspace(self, memory_id: str, workspace_id: str) -> Dict[str, Any]:
        """
        æŒ‰workspaceåˆ é™¤è®°å¿†ï¼ˆReMeç‰¹å®šæ–¹æ³•ï¼‰
        
        Args:
            memory_id: è®°å¿†ID
            workspace_id: workspace ID
            
        Returns:
            åˆ é™¤ç»“æœ
        """
        self.vector_store.delete([memory_id], workspace_id)
        
        # è‡ªåŠ¨ä¿å­˜workspace
        self.vector_store.dump_workspace(workspace_id, path=self.store_dir)
        
        self.logger.info(f"åˆ é™¤è®°å¿†: workspace={workspace_id}, id={memory_id} (å·²ä¿å­˜)")
        
        return {
            'status': 'success',
            'message': f'å·²åˆ é™¤è®°å¿† {memory_id}'
        }
    
    def get_all(self, user_id: str, **kwargs) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰è®°å¿†"""
        workspace_id = self._get_workspace_id(user_id)
        
        # ğŸ”§ å…ˆå°è¯•åŠ è½½å·²æœ‰è®°å¿†
        self._load_workspace_if_exists(workspace_id)
        
        # æ£€æŸ¥workspaceæ˜¯å¦å­˜åœ¨
        if not self.vector_store.exist_workspace(workspace_id):
            return {'results': []}
        
        # ä½¿ç”¨ iter_workspace_nodes() æ–¹æ³•è·å–workspaceä¸­çš„æ‰€æœ‰èŠ‚ç‚¹
        # è¿™æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œéœ€è¦è½¬æ¢ä¸ºåˆ—è¡¨
        workspace_nodes = list(self.vector_store.iter_workspace_nodes(workspace_id))
        
        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        results = []
        for node in workspace_nodes:
            result = {
                'id': node.unique_id,
                'memory': node.content,
                'metadata': node.metadata
            }
            results.append(result)
        
        self.logger.info(f"è·å–æ‰€æœ‰è®°å¿†: user={user_id}, å…±{len(results)}æ¡")
        
        return {'results': results}
    
    def reset(self, user_id: Optional[str] = None) -> Dict[str, Any]:
        """é‡ç½®è®°å¿†"""
        if user_id:
            workspace_id = self._get_workspace_id(user_id)
            if self.vector_store.exist_workspace(workspace_id):
                self.vector_store.delete_workspace(workspace_id)
                # æ³¨æ„ï¼šåˆ é™¤workspaceåæ— éœ€dumpï¼Œå› ä¸ºworkspaceå·²ä¸å­˜åœ¨
                self.logger.info(f"é‡ç½®è®°å¿†: user={user_id} (å·²åˆ é™¤workspace)")
                return {
                    'status': 'success',
                    'message': f'å·²é‡ç½® {user_id} çš„è®°å¿†'
                }
            else:
                return {
                    'status': 'success',
                    'message': f'{user_id} æ²¡æœ‰è®°å¿†éœ€è¦é‡ç½®'
                }
        else:
            # é‡ç½®æ‰€æœ‰ï¼ˆéœ€è¦çŸ¥é“æ‰€æœ‰workspaceï¼‰
            self.logger.warning("ReMeæ¡†æ¶é‡ç½®æ‰€æœ‰è®°å¿†éœ€è¦æ‰‹åŠ¨å¤„ç†")
            return {
                'status': 'partial_support',
                'message': 'è¯·æŒ‡å®šuser_idè¿›è¡Œé‡ç½®'
            }
    
    def get_framework_name(self) -> str:
        """è·å–æ¡†æ¶åç§°"""
        return "reme"
    
    def export_workspace(self, user_id: str, export_path: Optional[str] = None) -> Dict[str, Any]:
        """
        å¯¼å‡ºworkspaceï¼ˆReMeç‰¹å®šåŠŸèƒ½ï¼‰
        
        Args:
            user_id: ç”¨æˆ·ID
            export_path: å¯¼å‡ºè·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
            
        Returns:
            å¯¼å‡ºç»“æœ
        """
        workspace_id = self._get_workspace_id(user_id)
        
        if not export_path:
            export_path = os.path.join(self.store_dir, f"{workspace_id}.jsonl")
        
        result = self.vector_store.dump_workspace(workspace_id, path=export_path)
        
        self.logger.info(f"å¯¼å‡ºworkspace: {workspace_id} -> {export_path}")
        
        return {
            'status': 'success',
            'export_path': export_path,
            'result': result
        }
    
    def import_workspace(self, user_id: str, import_path: str) -> Dict[str, Any]:
        """
        å¯¼å…¥workspaceï¼ˆReMeç‰¹å®šåŠŸèƒ½ï¼‰
        
        Args:
            user_id: ç”¨æˆ·ID
            import_path: å¯¼å…¥è·¯å¾„
            
        Returns:
            å¯¼å…¥ç»“æœ
        """
        workspace_id = self._get_workspace_id(user_id)
        
        result = self.vector_store.load_workspace(workspace_id, path=import_path)
        
        self.logger.info(f"å¯¼å…¥workspace: {import_path} -> {workspace_id}")
        
        return {
            'status': 'success',
            'import_path': import_path,
            'result': result
        }

